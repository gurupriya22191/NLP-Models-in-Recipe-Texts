{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11251435,"sourceType":"datasetVersion","datasetId":7030993}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n# Path to the dataset directory\ndataset_path = '/kaggle/input/dataset9'\n\n# List all files inside the dataset folder\nfiles = os.listdir(dataset_path)\nprint(\"Files in dataset9:\", files)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:33:36.502187Z","iopub.execute_input":"2025-04-02T15:33:36.502397Z","iopub.status.idle":"2025-04-02T15:33:36.514627Z","shell.execute_reply.started":"2025-04-02T15:33:36.502376Z","shell.execute_reply":"2025-04-02T15:33:36.513749Z"}},"outputs":[{"name":"stdout","text":"Files in dataset9: ['final_mapped_categories.csv']\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"✅ Using device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:34:12.411138Z","iopub.execute_input":"2025-04-02T15:34:12.411419Z","iopub.status.idle":"2025-04-02T15:34:17.307601Z","shell.execute_reply.started":"2025-04-02T15:34:12.411396Z","shell.execute_reply":"2025-04-02T15:34:17.306792Z"}},"outputs":[{"name":"stdout","text":"✅ Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv(f'{dataset_path}/final_mapped_categories.csv')\n\n# Show first 5 rows\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:34:20.100489Z","iopub.execute_input":"2025-04-02T15:34:20.101033Z","iopub.status.idle":"2025-04-02T15:34:21.610432Z","shell.execute_reply.started":"2025-04-02T15:34:20.100998Z","shell.execute_reply":"2025-04-02T15:34:21.609669Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                          Recipe URL  \\\n0  https://www.food.com/recipe/cheesy-grits-casse...   \n1  https://www.food.com/recipe/crock-pot-beer-bra...   \n2  https://www.food.com/recipe/banana-bread-with-...   \n3  https://www.food.com/recipe/pasta-salad-suprem...   \n4  https://www.food.com/recipe/bread-machine-monk...   \n\n                        Recipe name  Preparation time  Servings  \\\n0            cheesy grits casserole              65.0       6.0   \n1              crock pot beer brats             440.0       4.0   \n2    banana bread with coconut milk              40.0      10.0   \n3               pasta salad supreme              25.0       6.0   \n4  bread machine monkey pull aparts              70.0      11.0   \n\n                                 List of ingredients  \\\n0  4cupswater 1teaspoonsalt 1cupquickcooking grit...   \n1  8bratwursts 2tablespoonsolive oil 2garlic clov...   \n2  4ripe mashedbananas 712ounces canscoconut milk...   \n3  16ouncesrotini pastaor 16 ouncesshell pastasom...   \n4  112teaspoonsactive dry yeast 114cupsbread flou...   \n\n                                List of instructions      Category  \\\n0  heat water and salt to boiling gradually add g...          oven   \n1  heat oil and garlic in a large skillet add bra...          pork   \n2  mix wet ingredients and dry ingredients mixtur...  quick breads   \n3  cook pasta rinse under cold water and drain we...     vegetable   \n4  add ingredients to your bread maker according ...        breads   \n\n  Region/Cuisine  Calories_Per_Serving Broad_Category  \n0        Unknown                 493.0          Bread  \n1        Unknown                 493.0   Meat/Poultry  \n2        Unknown                 493.0          Bread  \n3        Unknown                 493.0      Vegetable  \n4        Unknown                 493.0          Bread  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Recipe URL</th>\n      <th>Recipe name</th>\n      <th>Preparation time</th>\n      <th>Servings</th>\n      <th>List of ingredients</th>\n      <th>List of instructions</th>\n      <th>Category</th>\n      <th>Region/Cuisine</th>\n      <th>Calories_Per_Serving</th>\n      <th>Broad_Category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://www.food.com/recipe/cheesy-grits-casse...</td>\n      <td>cheesy grits casserole</td>\n      <td>65.0</td>\n      <td>6.0</td>\n      <td>4cupswater 1teaspoonsalt 1cupquickcooking grit...</td>\n      <td>heat water and salt to boiling gradually add g...</td>\n      <td>oven</td>\n      <td>Unknown</td>\n      <td>493.0</td>\n      <td>Bread</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://www.food.com/recipe/crock-pot-beer-bra...</td>\n      <td>crock pot beer brats</td>\n      <td>440.0</td>\n      <td>4.0</td>\n      <td>8bratwursts 2tablespoonsolive oil 2garlic clov...</td>\n      <td>heat oil and garlic in a large skillet add bra...</td>\n      <td>pork</td>\n      <td>Unknown</td>\n      <td>493.0</td>\n      <td>Meat/Poultry</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://www.food.com/recipe/banana-bread-with-...</td>\n      <td>banana bread with coconut milk</td>\n      <td>40.0</td>\n      <td>10.0</td>\n      <td>4ripe mashedbananas 712ounces canscoconut milk...</td>\n      <td>mix wet ingredients and dry ingredients mixtur...</td>\n      <td>quick breads</td>\n      <td>Unknown</td>\n      <td>493.0</td>\n      <td>Bread</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://www.food.com/recipe/pasta-salad-suprem...</td>\n      <td>pasta salad supreme</td>\n      <td>25.0</td>\n      <td>6.0</td>\n      <td>16ouncesrotini pastaor 16 ouncesshell pastasom...</td>\n      <td>cook pasta rinse under cold water and drain we...</td>\n      <td>vegetable</td>\n      <td>Unknown</td>\n      <td>493.0</td>\n      <td>Vegetable</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://www.food.com/recipe/bread-machine-monk...</td>\n      <td>bread machine monkey pull aparts</td>\n      <td>70.0</td>\n      <td>11.0</td>\n      <td>112teaspoonsactive dry yeast 114cupsbread flou...</td>\n      <td>add ingredients to your bread maker according ...</td>\n      <td>breads</td>\n      <td>Unknown</td>\n      <td>493.0</td>\n      <td>Bread</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\n\ndf['text'] = df['Recipe name'].fillna('') + ' ' + \\\n             df['List of ingredients'].fillna('') + ' ' + \\\n             df['List of instructions'].fillna('')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:51:21.320319Z","iopub.execute_input":"2025-04-02T13:51:21.320913Z","iopub.status.idle":"2025-04-02T13:51:21.398780Z","shell.execute_reply.started":"2025-04-02T13:51:21.320864Z","shell.execute_reply":"2025-04-02T13:51:21.397849Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny = le.fit_transform(df['Category'].fillna('Unknown'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:51:21.399670Z","iopub.execute_input":"2025-04-02T13:51:21.399903Z","iopub.status.idle":"2025-04-02T13:51:21.417224Z","shell.execute_reply.started":"2025-04-02T13:51:21.399874Z","shell.execute_reply":"2025-04-02T13:51:21.416376Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\nmodel.to(device)  # 🚨 Important for GPU!\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:51:21.418150Z","iopub.execute_input":"2025-04-02T13:51:21.418437Z","iopub.status.idle":"2025-04-02T13:51:22.294956Z","shell.execute_reply.started":"2025-04-02T13:51:21.418407Z","shell.execute_reply":"2025-04-02T13:51:22.294081Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x BertLayer(\n        (attention): BertAttention(\n          (self): BertSdpaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\n\ndef get_bert_embeddings_batch(texts, batch_size=32):\n    embeddings = []\n    for i in tqdm(range(0, len(texts), batch_size)):\n        batch_texts = texts[i:i + batch_size]\n        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True,\n                           truncation=True, max_length=128).to(device)\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # Move back to CPU\n        embeddings.extend(cls_embeddings)\n    return np.array(embeddings)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:51:22.295813Z","iopub.execute_input":"2025-04-02T13:51:22.296083Z","iopub.status.idle":"2025-04-02T13:51:22.301301Z","shell.execute_reply.started":"2025-04-02T13:51:22.296052Z","shell.execute_reply":"2025-04-02T13:51:22.300248Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"bert_features = get_bert_embeddings_batch(df['text'].tolist(), batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:51:22.303470Z","iopub.execute_input":"2025-04-02T13:51:22.303706Z","iopub.status.idle":"2025-04-02T13:55:23.599781Z","shell.execute_reply.started":"2025-04-02T13:51:22.303685Z","shell.execute_reply":"2025-04-02T13:55:23.598544Z"}},"outputs":[{"name":"stderr","text":" 57%|█████▋    | 896/1563 [04:01<02:59,  3.71it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-fae9400fbbbb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bert_embeddings_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-27-d999c50b09a5>\u001b[0m in \u001b[0;36mget_bert_embeddings_batch\u001b[0;34m(texts, batch_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mcls_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Move back to CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":28},{"cell_type":"code","source":"import joblib\njoblib.dump(bert_features, \"bert_features.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:55:23.601118Z","iopub.status.idle":"2025-04-02T13:55:23.601550Z","shell.execute_reply":"2025-04-02T13:55:23.601365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# 🌟 Check for GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ✨ Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    bert_features, y, test_size=0.2, random_state=42\n)\n\n# 🌟 Convert to tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n\n# 🔢 Determine number of output classes\nnum_classes = len(set(y))  # e.g., 2 for binary\n\n# 🔧 Define model (Linear = Logistic Regression)\nmodel = nn.Sequential(\n    nn.Linear(X_train.shape[1], num_classes)  # 768 → classes\n).to(device)\n\n# 🔥 Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# 🧠 Training loop\nepochs = 100\nfor epoch in range(epochs):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(X_train_tensor)\n    loss = criterion(outputs, y_train_tensor)\n    loss.backward()\n    optimizer.step()\n    \n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:55:23.602686Z","iopub.status.idle":"2025-04-02T13:55:23.603080Z","shell.execute_reply":"2025-04-02T13:55:23.602896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🎯 Evaluation\nmodel.eval()\nwith torch.no_grad():\n    predictions = model(X_test_tensor)\n    predicted_labels = torch.argmax(predictions, axis=1)\n\naccuracy = (predicted_labels == y_test_tensor).float().mean()\nprint(f\"Test Accuracy: {accuracy.item() * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:55:23.603868Z","iopub.status.idle":"2025-04-02T13:55:23.604262Z","shell.execute_reply":"2025-04-02T13:55:23.604099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Track loss over epochs\nlosses = []\n\nfor epoch in range(epochs):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(X_train_tensor)\n    loss = criterion(outputs, y_train_tensor)\n    loss.backward()\n    optimizer.step()\n    \n    losses.append(loss.item())\n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")\n        \n# Plotting\nimport matplotlib.pyplot as plt\n\nplt.plot(losses)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss Over Time\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:55:23.605257Z","iopub.status.idle":"2025-04-02T13:55:23.605622Z","shell.execute_reply":"2025-04-02T13:55:23.605462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\njoblib.dump(le, \"/kaggle/working/label_encoder.pkl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:55:23.606636Z","iopub.status.idle":"2025-04-02T13:55:23.607063Z","shell.execute_reply":"2025-04-02T13:55:23.606866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🔐 Save model weights\ntorch.save(model.state_dict(), \"/kaggle/working/mlp_model.pt\")\nprint(\"✅ Model saved successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:55:23.607830Z","iopub.status.idle":"2025-04-02T13:55:23.608216Z","shell.execute_reply":"2025-04-02T13:55:23.608050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nif torch.is_tensor(bert_features):\n    bert_features = bert_features.cpu().numpy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:55:23.609060Z","iopub.status.idle":"2025-04-02T13:55:23.609424Z","shell.execute_reply":"2025-04-02T13:55:23.609264Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:55:23.610162Z","iopub.status.idle":"2025-04-02T13:55:23.610525Z","shell.execute_reply":"2025-04-02T13:55:23.610362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nfrom sklearn.preprocessing import LabelEncoder\n\n# 👇 Load the file\nwith open('/kaggle/working/label_encoder.pkl', 'rb') as f:\n    obj = pickle.load(f)\n\n# 🧠 Check what was saved\nif isinstance(obj, LabelEncoder):\n    le = obj\n    print(\"✅ Loaded full LabelEncoder\")\nelif isinstance(obj, (list, tuple, np.ndarray)):\n    le = LabelEncoder()\n    le.classes_ = obj\n    print(\"⚠️ Loaded class array, recreated LabelEncoder\")\nelse:\n    raise ValueError(\"❌ Unknown label_encoder.pkl format!\")\n\n# ✅ Get number of classes for model\nnum_labels = len(le.classes_)\nprint(\"Number of labels:\", num_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:58:21.056774Z","iopub.execute_input":"2025-04-02T13:58:21.057136Z","iopub.status.idle":"2025-04-02T13:58:21.063758Z","shell.execute_reply.started":"2025-04-02T13:58:21.057104Z","shell.execute_reply":"2025-04-02T13:58:21.062866Z"}},"outputs":[{"name":"stdout","text":"⚠️ Loaded class array, recreated LabelEncoder\nNumber of labels: 591\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"\n# Combine text fields\ndf['text'] = df['Recipe name'].fillna('') + ' ' + \\\n             df['List of ingredients'].fillna('') + ' ' + \\\n             df['List of instructions'].fillna('')\n\n# Apply loaded LabelEncoder\ndf['label'] = le.transform(df['Category'].fillna('Unknown'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:59:31.185540Z","iopub.execute_input":"2025-04-02T13:59:31.185828Z","iopub.status.idle":"2025-04-02T13:59:31.289538Z","shell.execute_reply.started":"2025-04-02T13:59:31.185805Z","shell.execute_reply":"2025-04-02T13:59:31.288842Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom datasets import Dataset\n\ntrain_df, test_df = train_test_split(df[['text', 'label']], test_size=0.2, random_state=42)\ntrain_ds = Dataset.from_pandas(train_df)\ntest_ds = Dataset.from_pandas(test_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:59:41.399574Z","iopub.execute_input":"2025-04-02T13:59:41.399866Z","iopub.status.idle":"2025-04-02T13:59:42.766188Z","shell.execute_reply.started":"2025-04-02T13:59:41.399842Z","shell.execute_reply":"2025-04-02T13:59:42.765260Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef tokenize_fn(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n\ntrain_ds = train_ds.map(tokenize_fn, batched=True)\ntest_ds = test_ds.map(tokenize_fn, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:59:52.833824Z","iopub.execute_input":"2025-04-02T13:59:52.834165Z","iopub.status.idle":"2025-04-02T14:02:31.405955Z","shell.execute_reply.started":"2025-04-02T13:59:52.834136Z","shell.execute_reply":"2025-04-02T14:02:31.405075Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40009 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1bfb0c4c8af4c2590c6afbeeb157c9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10003 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d14daf74e224ace97551aeb2e4d745f"}},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"from transformers import BertForSequenceClassification\n\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T14:03:07.037319Z","iopub.execute_input":"2025-04-02T14:03:07.037637Z","iopub.status.idle":"2025-04-02T14:03:07.390074Z","shell.execute_reply.started":"2025-04-02T14:03:07.037601Z","shell.execute_reply":"2025-04-02T14:03:07.389380Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    num_train_epochs=4,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    logging_steps=50\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=test_ds,\n    tokenizer=tokenizer\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T14:05:04.154440Z","iopub.execute_input":"2025-04-02T14:05:04.154759Z","iopub.status.idle":"2025-04-02T14:05:09.417315Z","shell.execute_reply.started":"2025-04-02T14:05:04.154732Z","shell.execute_reply":"2025-04-02T14:05:09.416664Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-39-9ab9ba8a0af5>:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T14:05:21.825695Z","iopub.execute_input":"2025-04-02T14:05:21.825998Z","execution_failed":"2025-04-02T15:33:04.710Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"eval_results = trainer.evaluate()\nprint(\"📈 Evaluation Results:\", eval_results)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-02T15:33:04.710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\npreds = trainer.predict(test_ds)\npred_labels = preds.predictions.argmax(-1)\n\nprint(classification_report(test_df['label'], pred_labels, target_names=le.classes_))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# 📦 Install required libraries\n!pip install transformers datasets --quiet\n\n# 🔽 Imports\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset\nimport pickle\n\n\n\n# 🧠 Combine text fields\ndf['text'] = df['Recipe name'].fillna('') + ' ' + \\\n             df['List of ingredients'].fillna('') + ' ' + \\\n             df['List of instructions'].fillna('')\n\n# 🏷️ Encode target labels\nle = LabelEncoder()\ndf['label'] = le.fit_transform(df['Category'].fillna('Unknown'))\nnum_labels = len(le.classes_)\n\n# ✂️ Train-test split\ntrain_df, test_df = train_test_split(df[['text', 'label']], test_size=0.2, random_state=42)\n\n# 🔄 Convert to HuggingFace Dataset\ntrain_ds = Dataset.from_pandas(train_df)\ntest_ds = Dataset.from_pandas(test_df)\n\n# ✏️ Tokenize\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef tokenize_fn(batch):\n    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n\ntrain_ds = train_ds.map(tokenize_fn, batched=True)\ntest_ds = test_ds.map(tokenize_fn, batched=True)\n\ntrain_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\ntest_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n\n# 🔧 Load BERT model\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n\n# ⚙️ Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",  # Replaces deprecated evaluation_strategy\n    save_strategy=\"no\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    num_train_epochs=4,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    disable_tqdm=False\n)\n\n# 🧠 Trainer setup\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=test_ds,\n    tokenizer=tokenizer\n)\n\n# 🚀 Train the model\ntrainer.train()\n\n# 📊 Evaluate\neval_results = trainer.evaluate()\nprint(\"📈 Evaluation:\", eval_results)\n\n# 💾 Save model and label encoder\nmodel.save_pretrained(\"/kaggle/working/bert_finetuned/\")\ntokenizer.save_pretrained(\"/kaggle/working/bert_finetuned/\")\nwith open(\"/kaggle/working/label_encoder.pkl\", \"wb\") as f:\n    pickle.dump(le, f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:34:32.258820Z","iopub.execute_input":"2025-04-02T15:34:32.259157Z","execution_failed":"2025-04-02T20:58:34.584Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57fdba4d0112466ea45ffdc25a3cdc1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f7fd23de169417399278a00a7441710"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac2d8ac4e8ee4d24aba370724a0cff3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af3ffc87fe1d4885b1b0c2e8faa196b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40009 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94d3d43ac3844ed5830377f291b9822b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10003 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c5138c4c35b442fae2ff43476181575"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b25a53afbb61444e97c4e92b480b40c3"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-4-d810f935d5a3>:64: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom transformers import get_scheduler\nfrom tqdm import tqdm\n\n# 📌 Check GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"🔥 Using device:\", device)\n\n# 📁 Load dataset\ndf = pd.read_csv('/kaggle/input/dataset9/final_mapped_categories.csv')\ndf['text'] = df['Recipe name'].fillna('') + ' ' + df['List of ingredients'].fillna('') + ' ' + df['List of instructions'].fillna('')\n\n# 🔠 Encode labels\nle = LabelEncoder()\ndf['label'] = le.fit_transform(df['Category'].fillna('Unknown'))\n\n\n# 🚫 Remove labels with <2 samples\nlabel_counts = df['label'].value_counts()\nvalid_labels = label_counts[label_counts > 1].index\ndf = df[df['label'].isin(valid_labels)]\n\n\n# ✂️ Train-test split\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    df['text'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42, stratify=df['label']\n)\n\n# 🧙‍♂️ Tokenize\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n\n# 📦 Dataset class\nclass RecipeDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.encodings['input_ids'][idx],\n            'attention_mask': self.encodings['attention_mask'][idx],\n            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\ntrain_dataset = RecipeDataset(train_encodings, train_labels)\nval_dataset = RecipeDataset(val_encodings, val_labels)\n\n# 📚 Dataloaders\ntrain_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)\nval_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=16)\n\n# 🧠 Load BERT with classification head\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(le.classes_))\nmodel.to(device)\n\n# ⚙️ Optimizer & Scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5)\nnum_training_steps = len(train_loader) * 4  # 4 epochs\nlr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\n# 🚀 Training loop\nEPOCHS = 4\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss = 0\n\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n        total_loss += loss.item()\n\n    print(f\"📉 Epoch {epoch+1} Loss: {total_loss / len(train_loader):.4f}\")\n\n    # ✅ Validation\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            predictions = torch.argmax(outputs.logits, dim=1)\n            correct += (predictions == batch['labels']).sum().item()\n            total += batch['labels'].size(0)\n\n    print(f\"✅ Epoch {epoch+1} Validation Accuracy: {100 * correct / total:.2f}%\")\n\nprint(\"🎉 Training complete! Krishna-Radha blessings on your BERT model 🦚✨\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:10:13.258034Z","iopub.execute_input":"2025-04-03T05:10:13.258360Z","iopub.status.idle":"2025-04-03T06:19:21.719736Z","shell.execute_reply.started":"2025-04-03T05:10:13.258333Z","shell.execute_reply":"2025-04-03T06:19:21.718876Z"}},"outputs":[{"name":"stdout","text":"🔥 Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c81242dd8de849bd9700279ea641db20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdfe96ee159b4d2eb9431fa207c2cd7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9b33f1b5d8342c5aa067b97b2a57459"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3370b1e72eb74828949605fdffdaff70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3eba50e0b6a4642847ae6edf1a0f1e2"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1 Training: 100%|██████████| 2491/2491 [15:15<00:00,  2.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Epoch 1 Loss: 2.8029\n✅ Epoch 1 Validation Accuracy: 48.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|██████████| 2491/2491 [15:23<00:00,  2.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Epoch 2 Loss: 1.9071\n✅ Epoch 2 Validation Accuracy: 53.42%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Training: 100%|██████████| 2491/2491 [15:22<00:00,  2.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Epoch 3 Loss: 1.6241\n✅ Epoch 3 Validation Accuracy: 54.99%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 Training: 100%|██████████| 2491/2491 [15:22<00:00,  2.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Epoch 4 Loss: 1.4666\n✅ Epoch 4 Validation Accuracy: 55.97%\n🎉 Training complete! Krishna-Radha blessings on your BERT model 🦚✨\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 💾 Save the model\nmodel.save_pretrained(\"bert_recipe_classifier\")\ntokenizer.save_pretrained(\"bert_recipe_classifier\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:23:38.113401Z","iopub.execute_input":"2025-04-03T06:23:38.113721Z","iopub.status.idle":"2025-04-03T06:23:39.171255Z","shell.execute_reply.started":"2025-04-03T06:23:38.113698Z","shell.execute_reply":"2025-04-03T06:23:39.170607Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"('bert_recipe_classifier/tokenizer_config.json',\n 'bert_recipe_classifier/special_tokens_map.json',\n 'bert_recipe_classifier/vocab.txt',\n 'bert_recipe_classifier/added_tokens.json')"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification\n\n# 🔁 Load your saved model and tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert_recipe_classifier\")\nmodel = BertForSequenceClassification.from_pretrained(\"bert_recipe_classifier\")\nmodel.to(device)  # ⚡ Send it to GPU again\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:24:02.810653Z","iopub.execute_input":"2025-04-03T06:24:02.810935Z","iopub.status.idle":"2025-04-03T06:24:03.063141Z","shell.execute_reply.started":"2025-04-03T06:24:02.810913Z","shell.execute_reply":"2025-04-03T06:24:03.062457Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=591, bias=True)\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"EPOCHS = 9\nstart_epoch = 4  # Resume from here\n\nfor epoch in range(start_epoch, EPOCHS):\n    model.train()\n    total_loss = 0\n    correct_train = 0\n    total_train = 0\n\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=1)\n\n        # ✅ Track training accuracy\n        correct_train += (predictions == batch['labels']).sum().item()\n        total_train += batch['labels'].size(0)\n\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n        total_loss += loss.item()\n\n    train_acc = 100 * correct_train / total_train\n    print(f\"📉 Epoch {epoch+1} Loss: {total_loss / len(train_loader):.4f}\")\n    print(f\"✅ Epoch {epoch+1} Training Accuracy: {train_acc:.2f}%\")\n\n    # 🧪 Validation\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            predictions = torch.argmax(outputs.logits, dim=1)\n            correct += (predictions == batch['labels']).sum().item()\n            total += batch['labels'].size(0)\n\n    val_acc = 100 * correct / total\n    print(f\"🧪 Epoch {epoch+1} Validation Accuracy: {val_acc:.2f}%\\n\")\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:43:49.996879Z","iopub.execute_input":"2025-04-03T06:43:49.997168Z","iopub.status.idle":"2025-04-03T08:01:51.458070Z","shell.execute_reply.started":"2025-04-03T06:43:49.997145Z","shell.execute_reply":"2025-04-03T08:01:51.457361Z"}},"outputs":[{"name":"stderr","text":"Epoch 5 Training: 100%|██████████| 2491/2491 [14:21<00:00,  2.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Epoch 5 Loss: 1.4134\n✅ Epoch 5 Training Accuracy: 62.81%\n🧪 Epoch 5 Validation Accuracy: 55.97%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6 Training: 100%|██████████| 2491/2491 [14:20<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Epoch 6 Loss: 1.4105\n✅ Epoch 6 Training Accuracy: 63.07%\n🧪 Epoch 6 Validation Accuracy: 55.97%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7 Training: 100%|██████████| 2491/2491 [14:19<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Epoch 7 Loss: 1.4112\n✅ Epoch 7 Training Accuracy: 62.78%\n🧪 Epoch 7 Validation Accuracy: 55.97%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8 Training: 100%|██████████| 2491/2491 [14:19<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Epoch 8 Loss: 1.4132\n✅ Epoch 8 Training Accuracy: 63.08%\n🧪 Epoch 8 Validation Accuracy: 55.97%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9 Training: 100%|██████████| 2491/2491 [14:20<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Epoch 9 Loss: 1.4123\n✅ Epoch 9 Training Accuracy: 63.09%\n🧪 Epoch 9 Validation Accuracy: 55.97%\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}