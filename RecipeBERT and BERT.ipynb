{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11251435,"sourceType":"datasetVersion","datasetId":7030993}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\n# Path to the dataset directory\ndataset_path = '/kaggle/input/dataset9'\n\n# List all files inside the dataset folder\nfiles = os.listdir(dataset_path)\nprint(\"Files in dataset9:\", files)\n\n# Load the CSV file\ndf = pd.read_csv(f'{dataset_path}/final_mapped_categories.csv')\n\n# Show first 5 rows\ndf.head()\n\n\nimport torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"✅ Using device:\", device)\nimport pandas as pd\n\ndf['text'] = df['Recipe name'].fillna('') + ' ' + \\\n             df['List of ingredients'].fillna('') + ' ' + \\\n             df['List of instructions'].fillna('')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T22:35:15.524137Z","iopub.execute_input":"2025-04-16T22:35:15.524450Z","iopub.status.idle":"2025-04-16T22:35:20.786411Z","shell.execute_reply.started":"2025-04-16T22:35:15.524420Z","shell.execute_reply":"2025-04-16T22:35:20.785571Z"}},"outputs":[{"name":"stdout","text":"Files in dataset9: ['final_mapped_categories.csv']\n✅ Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ✅ DISABLE wandb logging globally before importing Trainer\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T06:07:10.488227Z","iopub.execute_input":"2025-04-15T06:07:10.488491Z","iopub.status.idle":"2025-04-15T06:07:10.492131Z","shell.execute_reply.started":"2025-04-15T06:07:10.488472Z","shell.execute_reply":"2025-04-15T06:07:10.491224Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T06:07:15.151792Z","iopub.execute_input":"2025-04-15T06:07:15.152112Z","iopub.status.idle":"2025-04-15T06:07:17.976529Z","shell.execute_reply.started":"2025-04-15T06:07:15.152089Z","shell.execute_reply":"2025-04-15T06:07:17.975646Z"}},"outputs":[{"name":"stdout","text":"✅ Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv(f'{dataset_path}/final_mapped_categories.csv')\n\n# Show first 5 rows\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T09:15:43.738670Z","iopub.execute_input":"2025-04-14T09:15:43.739004Z","iopub.status.idle":"2025-04-14T09:15:44.312233Z","shell.execute_reply.started":"2025-04-14T09:15:43.738976Z","shell.execute_reply":"2025-04-14T09:15:44.311234Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"                                          Recipe URL  \\\n0  https://www.food.com/recipe/cheesy-grits-casse...   \n1  https://www.food.com/recipe/crock-pot-beer-bra...   \n2  https://www.food.com/recipe/banana-bread-with-...   \n3  https://www.food.com/recipe/pasta-salad-suprem...   \n4  https://www.food.com/recipe/bread-machine-monk...   \n\n                        Recipe name  Preparation time  Servings  \\\n0            cheesy grits casserole              65.0       6.0   \n1              crock pot beer brats             440.0       4.0   \n2    banana bread with coconut milk              40.0      10.0   \n3               pasta salad supreme              25.0       6.0   \n4  bread machine monkey pull aparts              70.0      11.0   \n\n                                 List of ingredients  \\\n0  4cupswater 1teaspoonsalt 1cupquickcooking grit...   \n1  8bratwursts 2tablespoonsolive oil 2garlic clov...   \n2  4ripe mashedbananas 712ounces canscoconut milk...   \n3  16ouncesrotini pastaor 16 ouncesshell pastasom...   \n4  112teaspoonsactive dry yeast 114cupsbread flou...   \n\n                                List of instructions      Category  \\\n0  heat water and salt to boiling gradually add g...          oven   \n1  heat oil and garlic in a large skillet add bra...          pork   \n2  mix wet ingredients and dry ingredients mixtur...  quick breads   \n3  cook pasta rinse under cold water and drain we...     vegetable   \n4  add ingredients to your bread maker according ...        breads   \n\n  Region/Cuisine  Calories_Per_Serving Broad_Category  \n0        Unknown                 493.0          Bread  \n1        Unknown                 493.0   Meat/Poultry  \n2        Unknown                 493.0          Bread  \n3        Unknown                 493.0      Vegetable  \n4        Unknown                 493.0          Bread  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Recipe URL</th>\n      <th>Recipe name</th>\n      <th>Preparation time</th>\n      <th>Servings</th>\n      <th>List of ingredients</th>\n      <th>List of instructions</th>\n      <th>Category</th>\n      <th>Region/Cuisine</th>\n      <th>Calories_Per_Serving</th>\n      <th>Broad_Category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://www.food.com/recipe/cheesy-grits-casse...</td>\n      <td>cheesy grits casserole</td>\n      <td>65.0</td>\n      <td>6.0</td>\n      <td>4cupswater 1teaspoonsalt 1cupquickcooking grit...</td>\n      <td>heat water and salt to boiling gradually add g...</td>\n      <td>oven</td>\n      <td>Unknown</td>\n      <td>493.0</td>\n      <td>Bread</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://www.food.com/recipe/crock-pot-beer-bra...</td>\n      <td>crock pot beer brats</td>\n      <td>440.0</td>\n      <td>4.0</td>\n      <td>8bratwursts 2tablespoonsolive oil 2garlic clov...</td>\n      <td>heat oil and garlic in a large skillet add bra...</td>\n      <td>pork</td>\n      <td>Unknown</td>\n      <td>493.0</td>\n      <td>Meat/Poultry</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://www.food.com/recipe/banana-bread-with-...</td>\n      <td>banana bread with coconut milk</td>\n      <td>40.0</td>\n      <td>10.0</td>\n      <td>4ripe mashedbananas 712ounces canscoconut milk...</td>\n      <td>mix wet ingredients and dry ingredients mixtur...</td>\n      <td>quick breads</td>\n      <td>Unknown</td>\n      <td>493.0</td>\n      <td>Bread</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://www.food.com/recipe/pasta-salad-suprem...</td>\n      <td>pasta salad supreme</td>\n      <td>25.0</td>\n      <td>6.0</td>\n      <td>16ouncesrotini pastaor 16 ouncesshell pastasom...</td>\n      <td>cook pasta rinse under cold water and drain we...</td>\n      <td>vegetable</td>\n      <td>Unknown</td>\n      <td>493.0</td>\n      <td>Vegetable</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://www.food.com/recipe/bread-machine-monk...</td>\n      <td>bread machine monkey pull aparts</td>\n      <td>70.0</td>\n      <td>11.0</td>\n      <td>112teaspoonsactive dry yeast 114cupsbread flou...</td>\n      <td>add ingredients to your bread maker according ...</td>\n      <td>breads</td>\n      <td>Unknown</td>\n      <td>493.0</td>\n      <td>Bread</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny = le.fit_transform(df['Category'].fillna('Unknown'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T08:56:42.604153Z","iopub.execute_input":"2025-04-14T08:56:42.604508Z","iopub.status.idle":"2025-04-14T08:56:42.622329Z","shell.execute_reply.started":"2025-04-14T08:56:42.604482Z","shell.execute_reply":"2025-04-14T08:56:42.621640Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\nmodel.to(device)  # 🚨 Important for GPU!\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T08:56:47.650464Z","iopub.execute_input":"2025-04-14T08:56:47.650865Z","iopub.status.idle":"2025-04-14T08:56:48.160832Z","shell.execute_reply.started":"2025-04-14T08:56:47.650830Z","shell.execute_reply":"2025-04-14T08:56:48.159982Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x BertLayer(\n        (attention): BertAttention(\n          (self): BertSdpaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np\n\ndef get_bert_embeddings_batch(texts, batch_size=32):\n    embeddings = []\n    for i in tqdm(range(0, len(texts), batch_size)):\n        batch_texts = texts[i:i + batch_size]\n        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True,\n                           truncation=True, max_length=128).to(device)\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # Move back to CPU\n        embeddings.extend(cls_embeddings)\n    return np.array(embeddings)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:51:22.295813Z","iopub.execute_input":"2025-04-02T13:51:22.296083Z","iopub.status.idle":"2025-04-02T13:51:22.301301Z","shell.execute_reply.started":"2025-04-02T13:51:22.296052Z","shell.execute_reply":"2025-04-02T13:51:22.300248Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"bert_features = get_bert_embeddings_batch(df['text'].tolist(), batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:51:22.303470Z","iopub.execute_input":"2025-04-02T13:51:22.303706Z","iopub.status.idle":"2025-04-02T13:55:23.599781Z","shell.execute_reply.started":"2025-04-02T13:51:22.303685Z","shell.execute_reply":"2025-04-02T13:55:23.598544Z"}},"outputs":[{"name":"stderr","text":" 57%|█████▋    | 896/1563 [04:01<02:59,  3.71it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-fae9400fbbbb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bert_embeddings_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-27-d999c50b09a5>\u001b[0m in \u001b[0;36mget_bert_embeddings_batch\u001b[0;34m(texts, batch_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mcls_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Move back to CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":28},{"cell_type":"code","source":"import joblib\njoblib.dump(bert_features, \"bert_features.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:55:23.601118Z","iopub.status.idle":"2025-04-02T13:55:23.601550Z","shell.execute_reply":"2025-04-02T13:55:23.601365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# 🌟 Check for GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# ✨ Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    bert_features, y, test_size=0.2, random_state=42\n)\n\n# 🌟 Convert to tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n\n# 🔢 Determine number of output classes\nnum_classes = len(set(y))  # e.g., 2 for binary\n\n# 🔧 Define model (Linear = Logistic Regression)\nmodel = nn.Sequential(\n    nn.Linear(X_train.shape[1], num_classes)  # 768 → classes\n).to(device)\n\n# 🔥 Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# 🧠 Training loop\nepochs = 100\nfor epoch in range(epochs):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(X_train_tensor)\n    loss = criterion(outputs, y_train_tensor)\n    loss.backward()\n    optimizer.step()\n    \n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:55:23.602686Z","iopub.status.idle":"2025-04-02T13:55:23.603080Z","shell.execute_reply":"2025-04-02T13:55:23.602896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🎯 Evaluation\nmodel.eval()\nwith torch.no_grad():\n    predictions = model(X_test_tensor)\n    predicted_labels = torch.argmax(predictions, axis=1)\n\naccuracy = (predicted_labels == y_test_tensor).float().mean()\nprint(f\"Test Accuracy: {accuracy.item() * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:55:23.603868Z","iopub.status.idle":"2025-04-02T13:55:23.604262Z","shell.execute_reply":"2025-04-02T13:55:23.604099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Track loss over epochs\nlosses = []\n\nfor epoch in range(epochs):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(X_train_tensor)\n    loss = criterion(outputs, y_train_tensor)\n    loss.backward()\n    optimizer.step()\n    \n    losses.append(loss.item())\n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")\n        \n# Plotting\nimport matplotlib.pyplot as plt\n\nplt.plot(losses)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss Over Time\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:55:23.605257Z","iopub.status.idle":"2025-04-02T13:55:23.605622Z","shell.execute_reply":"2025-04-02T13:55:23.605462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\njoblib.dump(le, \"/kaggle/working/label_encoder.pkl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:55:23.606636Z","iopub.status.idle":"2025-04-02T13:55:23.607063Z","shell.execute_reply":"2025-04-02T13:55:23.606866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🔐 Save model weights\ntorch.save(model.state_dict(), \"/kaggle/working/mlp_model.pt\")\nprint(\"✅ Model saved successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:55:23.607830Z","iopub.status.idle":"2025-04-02T13:55:23.608216Z","shell.execute_reply":"2025-04-02T13:55:23.608050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nif torch.is_tensor(bert_features):\n    bert_features = bert_features.cpu().numpy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:55:23.609060Z","iopub.status.idle":"2025-04-02T13:55:23.609424Z","shell.execute_reply":"2025-04-02T13:55:23.609264Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:55:23.610162Z","iopub.status.idle":"2025-04-02T13:55:23.610525Z","shell.execute_reply":"2025-04-02T13:55:23.610362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nfrom sklearn.preprocessing import LabelEncoder\n\n# 👇 Load the file\nwith open('/kaggle/working/label_encoder.pkl', 'rb') as f:\n    obj = pickle.load(f)\n\n# 🧠 Check what was saved\nif isinstance(obj, LabelEncoder):\n    le = obj\n    print(\"✅ Loaded full LabelEncoder\")\nelif isinstance(obj, (list, tuple, np.ndarray)):\n    le = LabelEncoder()\n    le.classes_ = obj\n    print(\"⚠️ Loaded class array, recreated LabelEncoder\")\nelse:\n    raise ValueError(\"❌ Unknown label_encoder.pkl format!\")\n\n# ✅ Get number of classes for model\nnum_labels = len(le.classes_)\nprint(\"Number of labels:\", num_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:58:21.056774Z","iopub.execute_input":"2025-04-02T13:58:21.057136Z","iopub.status.idle":"2025-04-02T13:58:21.063758Z","shell.execute_reply.started":"2025-04-02T13:58:21.057104Z","shell.execute_reply":"2025-04-02T13:58:21.062866Z"}},"outputs":[{"name":"stdout","text":"⚠️ Loaded class array, recreated LabelEncoder\nNumber of labels: 591\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom datasets import Dataset\n\ntrain_df, test_df = train_test_split(df[['text', 'label']], test_size=0.2, random_state=42)\ntrain_ds = Dataset.from_pandas(train_df)\ntest_ds = Dataset.from_pandas(test_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:59:41.399574Z","iopub.execute_input":"2025-04-02T13:59:41.399866Z","iopub.status.idle":"2025-04-02T13:59:42.766188Z","shell.execute_reply.started":"2025-04-02T13:59:41.399842Z","shell.execute_reply":"2025-04-02T13:59:42.765260Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef tokenize_fn(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n\ntrain_ds = train_ds.map(tokenize_fn, batched=True)\ntest_ds = test_ds.map(tokenize_fn, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T13:59:52.833824Z","iopub.execute_input":"2025-04-02T13:59:52.834165Z","iopub.status.idle":"2025-04-02T14:02:31.405955Z","shell.execute_reply.started":"2025-04-02T13:59:52.834136Z","shell.execute_reply":"2025-04-02T14:02:31.405075Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40009 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1bfb0c4c8af4c2590c6afbeeb157c9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10003 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d14daf74e224ace97551aeb2e4d745f"}},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"from transformers import BertForSequenceClassification\n\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T14:03:07.037319Z","iopub.execute_input":"2025-04-02T14:03:07.037637Z","iopub.status.idle":"2025-04-02T14:03:07.390074Z","shell.execute_reply.started":"2025-04-02T14:03:07.037601Z","shell.execute_reply":"2025-04-02T14:03:07.389380Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    num_train_epochs=4,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    logging_steps=50\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=test_ds,\n    tokenizer=tokenizer\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T14:05:04.154440Z","iopub.execute_input":"2025-04-02T14:05:04.154759Z","iopub.status.idle":"2025-04-02T14:05:09.417315Z","shell.execute_reply.started":"2025-04-02T14:05:04.154732Z","shell.execute_reply":"2025-04-02T14:05:09.416664Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-39-9ab9ba8a0af5>:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T14:05:21.825695Z","iopub.execute_input":"2025-04-02T14:05:21.825998Z","execution_failed":"2025-04-02T15:33:04.710Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"eval_results = trainer.evaluate()\nprint(\"📈 Evaluation Results:\", eval_results)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-02T15:33:04.710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\npreds = trainer.predict(test_ds)\npred_labels = preds.predictions.argmax(-1)\n\nprint(classification_report(test_df['label'], pred_labels, target_names=le.classes_))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# 📦 Install required libraries\n!pip install transformers datasets --quiet\n\n# 🔽 Imports\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset\nimport pickle\n\n\n\n# 🧠 Combine text fields\ndf['text'] = df['Recipe name'].fillna('') + ' ' + \\\n             df['List of ingredients'].fillna('') + ' ' + \\\n             df['List of instructions'].fillna('')\n\n# 🏷️ Encode target labels\nle = LabelEncoder()\ndf['label'] = le.fit_transform(df['Category'].fillna('Unknown'))\nnum_labels = len(le.classes_)\n\n# ✂️ Train-test split\ntrain_df, test_df = train_test_split(df[['text', 'label']], test_size=0.2, random_state=42)\n\n# 🔄 Convert to HuggingFace Dataset\ntrain_ds = Dataset.from_pandas(train_df)\ntest_ds = Dataset.from_pandas(test_df)\n\n# ✏️ Tokenize\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef tokenize_fn(batch):\n    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n\ntrain_ds = train_ds.map(tokenize_fn, batched=True)\ntest_ds = test_ds.map(tokenize_fn, batched=True)\n\ntrain_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\ntest_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n\n# 🔧 Load BERT model\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n\n# ⚙️ Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",  # Replaces deprecated evaluation_strategy\n    save_strategy=\"no\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    num_train_epochs=4,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    disable_tqdm=False\n)\n\n# 🧠 Trainer setup\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=test_ds,\n    tokenizer=tokenizer\n)\n\n# 🚀 Train the model\ntrainer.train()\n\n# 📊 Evaluate\neval_results = trainer.evaluate()\nprint(\"📈 Evaluation:\", eval_results)\n\n# 💾 Save model and label encoder\nmodel.save_pretrained(\"/kaggle/working/bert_finetuned/\")\ntokenizer.save_pretrained(\"/kaggle/working/bert_finetuned/\")\nwith open(\"/kaggle/working/label_encoder.pkl\", \"wb\") as f:\n    pickle.dump(le, f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T15:34:32.258820Z","iopub.execute_input":"2025-04-02T15:34:32.259157Z","execution_failed":"2025-04-02T20:58:34.584Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57fdba4d0112466ea45ffdc25a3cdc1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f7fd23de169417399278a00a7441710"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac2d8ac4e8ee4d24aba370724a0cff3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af3ffc87fe1d4885b1b0c2e8faa196b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40009 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94d3d43ac3844ed5830377f291b9822b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10003 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c5138c4c35b442fae2ff43476181575"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b25a53afbb61444e97c4e92b480b40c3"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-4-d810f935d5a3>:64: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"**2.**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom transformers import get_scheduler\nfrom tqdm import tqdm\n\n# 📌 Check GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"🔥 Using device:\", device)\n\n# 📁 Load dataset\ndf = pd.read_csv('/kaggle/input/dataset9/final_mapped_categories.csv')\ndf['text'] = df['Recipe name'].fillna('') + ' ' + df['List of ingredients'].fillna('') + ' ' + df['List of instructions'].fillna('')\n\n# 🔠 Encode labels\nle = LabelEncoder()\ndf['label'] = le.fit_transform(df['Category'].fillna('Unknown'))\n\n\n# 🚫 Remove labels with <2 samples\nlabel_counts = df['label'].value_counts()\nvalid_labels = label_counts[label_counts > 1].index\ndf = df[df['label'].isin(valid_labels)]\n\n\n# ✂️ Train-test split\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    df['text'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42, stratify=df['label']\n)\n\n# 🧙‍♂️ Tokenize\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n\n# 📦 Dataset class\nclass RecipeDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.encodings['input_ids'][idx],\n            'attention_mask': self.encodings['attention_mask'][idx],\n            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\ntrain_dataset = RecipeDataset(train_encodings, train_labels)\nval_dataset = RecipeDataset(val_encodings, val_labels)\n\n# 📚 Dataloaders\ntrain_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)\nval_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=16)\n\n# 🧠 Load BERT with classification head\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(le.classes_))\nmodel.to(device)\n\n# ⚙️ Optimizer & Scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5)\nnum_training_steps = len(train_loader) * 4  # 4 epochs\nlr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\n# 🚀 Training loop\nEPOCHS = 4\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss = 0\n\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n        total_loss += loss.item()\n\n    print(f\"📉 Epoch {epoch+1} Loss: {total_loss / len(train_loader):.4f}\")\n\n    # ✅ Validation\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            predictions = torch.argmax(outputs.logits, dim=1)\n            correct += (predictions == batch['labels']).sum().item()\n            total += batch['labels'].size(0)\n\n    print(f\"✅ Epoch {epoch+1} Validation Accuracy: {100 * correct / total:.2f}%\")\n\nprint(\"🎉 Training complete! Krishna-Radha blessings on your BERT model 🦚✨\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T05:10:13.258034Z","iopub.execute_input":"2025-04-03T05:10:13.258360Z","iopub.status.idle":"2025-04-03T06:19:21.719736Z","shell.execute_reply.started":"2025-04-03T05:10:13.258333Z","shell.execute_reply":"2025-04-03T06:19:21.718876Z"}},"outputs":[{"name":"stdout","text":"🔥 Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c81242dd8de849bd9700279ea641db20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdfe96ee159b4d2eb9431fa207c2cd7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9b33f1b5d8342c5aa067b97b2a57459"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3370b1e72eb74828949605fdffdaff70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3eba50e0b6a4642847ae6edf1a0f1e2"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1 Training: 100%|██████████| 2491/2491 [15:15<00:00,  2.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Epoch 1 Loss: 2.8029\n✅ Epoch 1 Validation Accuracy: 48.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2 Training: 100%|██████████| 2491/2491 [15:23<00:00,  2.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Epoch 2 Loss: 1.9071\n✅ Epoch 2 Validation Accuracy: 53.42%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3 Training: 100%|██████████| 2491/2491 [15:22<00:00,  2.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Epoch 3 Loss: 1.6241\n✅ Epoch 3 Validation Accuracy: 54.99%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4 Training: 100%|██████████| 2491/2491 [15:22<00:00,  2.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Epoch 4 Loss: 1.4666\n✅ Epoch 4 Validation Accuracy: 55.97%\n🎉 Training complete! Krishna-Radha blessings on your BERT model 🦚✨\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 💾 Save the model\nmodel.save_pretrained(\"bert_recipe_classifier\")\ntokenizer.save_pretrained(\"bert_recipe_classifier\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:23:38.113401Z","iopub.execute_input":"2025-04-03T06:23:38.113721Z","iopub.status.idle":"2025-04-03T06:23:39.171255Z","shell.execute_reply.started":"2025-04-03T06:23:38.113698Z","shell.execute_reply":"2025-04-03T06:23:39.170607Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"('bert_recipe_classifier/tokenizer_config.json',\n 'bert_recipe_classifier/special_tokens_map.json',\n 'bert_recipe_classifier/vocab.txt',\n 'bert_recipe_classifier/added_tokens.json')"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification\n\n# 🔁 Load your saved model and tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert_recipe_classifier\")\nmodel = BertForSequenceClassification.from_pretrained(\"bert_recipe_classifier\")\nmodel.to(device)  # ⚡ Send it to GPU again\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:24:02.810653Z","iopub.execute_input":"2025-04-03T06:24:02.810935Z","iopub.status.idle":"2025-04-03T06:24:03.063141Z","shell.execute_reply.started":"2025-04-03T06:24:02.810913Z","shell.execute_reply":"2025-04-03T06:24:03.062457Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=591, bias=True)\n)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"EPOCHS = 9\nstart_epoch = 4  # Resume from here\n\nfor epoch in range(start_epoch, EPOCHS):\n    model.train()\n    total_loss = 0\n    correct_train = 0\n    total_train = 0\n\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=1)\n\n        # ✅ Track training accuracy\n        correct_train += (predictions == batch['labels']).sum().item()\n        total_train += batch['labels'].size(0)\n\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n        total_loss += loss.item()\n\n    train_acc = 100 * correct_train / total_train\n    print(f\"📉 Epoch {epoch+1} Loss: {total_loss / len(train_loader):.4f}\")\n    print(f\"✅ Epoch {epoch+1} Training Accuracy: {train_acc:.2f}%\")\n\n    # 🧪 Validation\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            predictions = torch.argmax(outputs.logits, dim=1)\n            correct += (predictions == batch['labels']).sum().item()\n            total += batch['labels'].size(0)\n\n    val_acc = 100 * correct / total\n    print(f\"🧪 Epoch {epoch+1} Validation Accuracy: {val_acc:.2f}%\\n\")\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T06:43:49.996879Z","iopub.execute_input":"2025-04-03T06:43:49.997168Z","iopub.status.idle":"2025-04-03T08:01:51.458070Z","shell.execute_reply.started":"2025-04-03T06:43:49.997145Z","shell.execute_reply":"2025-04-03T08:01:51.457361Z"}},"outputs":[{"name":"stderr","text":"Epoch 5 Training: 100%|██████████| 2491/2491 [14:21<00:00,  2.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Epoch 5 Loss: 1.4134\n✅ Epoch 5 Training Accuracy: 62.81%\n🧪 Epoch 5 Validation Accuracy: 55.97%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6 Training: 100%|██████████| 2491/2491 [14:20<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Epoch 6 Loss: 1.4105\n✅ Epoch 6 Training Accuracy: 63.07%\n🧪 Epoch 6 Validation Accuracy: 55.97%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7 Training: 100%|██████████| 2491/2491 [14:19<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Epoch 7 Loss: 1.4112\n✅ Epoch 7 Training Accuracy: 62.78%\n🧪 Epoch 7 Validation Accuracy: 55.97%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8 Training: 100%|██████████| 2491/2491 [14:19<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Epoch 8 Loss: 1.4132\n✅ Epoch 8 Training Accuracy: 63.08%\n🧪 Epoch 8 Validation Accuracy: 55.97%\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9 Training: 100%|██████████| 2491/2491 [14:20<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Epoch 9 Loss: 1.4123\n✅ Epoch 9 Training Accuracy: 63.09%\n🧪 Epoch 9 Validation Accuracy: 55.97%\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"for 10 april","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nfrom tqdm import tqdm\n\n# ✅ Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"🔥 Using device:\", device)\n\n# ✅ Combine recipe name + ingredients + instructions\ndf['text'] = df['Recipe name'].fillna('') + ' ' + df['List of ingredients'].fillna('') + ' ' + df['List of instructions'].fillna('')\n\n# ✅ Encode category\nle = LabelEncoder()\ndf['label'] = le.fit_transform(df['Category'])\n\n# ✅ Drop rare labels (less than 2 samples)\nlabel_counts = df['label'].value_counts()\nvalid_labels = label_counts[label_counts > 1].index\ndf = df[df['label'].isin(valid_labels)]\n\n# ✅ Train-test split\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    df['text'].tolist(), df['label'].tolist(), test_size=0.2, stratify=df['label'], random_state=42\n)\n\n# ✅ Tokenizer — if RecipeBERT fails, fallback to BERT\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(\"recipegpt/recipe-bert\")\n    model_name = \"recipegpt/recipe-bert\"\nexcept:\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n    model_name = \"bert-base-uncased\"\n    print(\"⚠️ RecipeBERT not found. Using BERT instead.\")\n\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256, return_tensors='pt')\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=256, return_tensors='pt')\n\n# ✅ Dataset class\nclass RecipeDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    def __len__(self):\n        return len(self.labels)\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.encodings['input_ids'][idx],\n            'attention_mask': self.encodings['attention_mask'][idx],\n            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\ntrain_dataset = RecipeDataset(train_encodings, train_labels)\nval_dataset = RecipeDataset(val_encodings, val_labels)\n\n# ✅ Load model\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(le.classes_))\nmodel.to(device)\n\n# ✅ DataLoader\ntrain_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)\nval_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=16)\n\n# ✅ Optimizer & Scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5)\nnum_training_steps = len(train_loader) * 5\nlr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\n# ✅ Training loop\nEPOCHS = 5\nfor epoch in range(EPOCHS):\n    print(f\"\\n🔁 Epoch {epoch+1}/{EPOCHS}\")\n    model.train()\n    total_loss = 0\n    correct_train = 0\n    total_train = 0\n\n    for batch in tqdm(train_loader, desc=\"Training\"):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=1)\n\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n        total_loss += loss.item()\n        correct_train += (predictions == batch['labels']).sum().item()\n        total_train += batch['labels'].size(0)\n\n    train_accuracy = 100 * correct_train / total_train\n    print(f\"📉 Train Loss: {total_loss/len(train_loader):.4f}\")\n    print(f\"✅ Train Accuracy: {train_accuracy:.2f}%\")\n\n    # ✅ Validation\n    model.eval()\n    correct_val = 0\n    total_val = 0\n    val_loss = 0\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validating\"):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            val_loss += outputs.loss.item()\n            predictions = torch.argmax(outputs.logits, dim=1)\n            correct_val += (predictions == batch['labels']).sum().item()\n            total_val += batch['labels'].size(0)\n\n    val_accuracy = 100 * correct_val / total_val\n    print(f\"🧪 Val Loss: {val_loss/len(val_loader):.4f}\")\n    print(f\"🧪 Val Accuracy: {val_accuracy:.2f}%\")\n\nprint(\"\\n🎉 Training complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T11:41:16.423332Z","iopub.execute_input":"2025-04-09T11:41:16.423864Z"}},"outputs":[{"name":"stdout","text":"🔥 Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a07789eed074ec68857923ade17771a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2e191c6ad224457975c47beb78eb4da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"956d3e2163614c6d84c150ce1451a59e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b2c3a1ac95f4c90bb5829a58e9c7c27"}},"metadata":{}},{"name":"stdout","text":"⚠️ RecipeBERT not found. Using BERT instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be1f777f44bf4efdbb0331ef06385ab4"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n🔁 Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2491/2491 [30:38<00:00,  1.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Train Loss: 2.8594\n✅ Train Accuracy: 38.03%\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 623/623 [02:30<00:00,  4.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"🧪 Val Loss: 2.1106\n🧪 Val Accuracy: 48.45%\n\n🔁 Epoch 2/5\n","output_type":"stream"},{"name":"stderr","text":"Training:  66%|██████▌   | 1646/2491 [20:22<10:27,  1.35it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nfrom tqdm import tqdm\n\n# ✅ Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"🔥 Using device:\", device)\n\n# ✅ Combine recipe name + ingredients + instructions\ndf['text'] = df['Recipe name'].fillna('') + ' ' + df['List of ingredients'].fillna('') + ' ' + df['List of instructions'].fillna('')\n\n# ✅ Encode category\nle = LabelEncoder()\ndf['label'] = le.fit_transform(df['Category'])\n\n# ✅ Drop rare labels (less than 2 samples)\nlabel_counts = df['label'].value_counts()\nvalid_labels = label_counts[label_counts > 1].index\ndf = df[df['label'].isin(valid_labels)]\n\n# ✅ Train-test split\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    df['text'].tolist(), df['label'].tolist(), test_size=0.2, stratify=df['label'], random_state=42\n)\n\n# ✅ Tokenizer — if RecipeBERT fails, fallback to BERT\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(\"recipegpt/recipe-bert\")\n    model_name = \"recipegpt/recipe-bert\"\nexcept:\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n    model_name = \"bert-base-uncased\"\n    print(\"⚠️ RecipeBERT not found. Using BERT instead.\")\n\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256, return_tensors='pt')\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=256, return_tensors='pt')\n\n# ✅ Dataset class\nclass RecipeDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    def __len__(self):\n        return len(self.labels)\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.encodings['input_ids'][idx],\n            'attention_mask': self.encodings['attention_mask'][idx],\n            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\ntrain_dataset = RecipeDataset(train_encodings, train_labels)\nval_dataset = RecipeDataset(val_encodings, val_labels)\n\n# ✅ Load model\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(le.classes_))\nmodel.to(device)\n\n# ✅ DataLoader\ntrain_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)\nval_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=16)\n\n# ✅ Optimizer & Scheduler\noptimizer = AdamW(model.parameters(), lr=2e-5)\nnum_training_steps = len(train_loader) * 5\nlr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\n# ✅ Training loop\nEPOCHS = 5\nfor epoch in range(EPOCHS):\n    print(f\"\\n🔁 Epoch {epoch+1}/{EPOCHS}\")\n    model.train()\n    total_loss = 0\n    correct_train = 0\n    total_train = 0\n\n    for batch in tqdm(train_loader, desc=\"Training\"):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=1)\n\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\n        total_loss += loss.item()\n        correct_train += (predictions == batch['labels']).sum().item()\n        total_train += batch['labels'].size(0)\n\n    train_accuracy = 100 * correct_train / total_train\n    print(f\"📉 Train Loss: {total_loss/len(train_loader):.4f}\")\n    print(f\"✅ Train Accuracy: {train_accuracy:.2f}%\")\n\n    # ✅ Validation\n    model.eval()\n    correct_val = 0\n    total_val = 0\n    val_loss = 0\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validating\"):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            val_loss += outputs.loss.item()\n            predictions = torch.argmax(outputs.logits, dim=1)\n            correct_val += (predictions == batch['labels']).sum().item()\n            total_val += batch['labels'].size(0)\n\n    val_accuracy = 100 * correct_val / total_val\n    print(f\"🧪 Val Loss: {val_loss/len(val_loader):.4f}\")\n    print(f\"🧪 Val Accuracy: {val_accuracy:.2f}%\")\n\nprint(\"\\n🎉 Training complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T17:34:51.449756Z","iopub.execute_input":"2025-04-09T17:34:51.450065Z"}},"outputs":[{"name":"stdout","text":"🔥 Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"525a2ed947404d83a1323ea93974b9b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00815997b9804787871254030fddf912"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81066ba88cae401586488e28774c4775"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11ce6fa89bf4498497fa39b60efdba59"}},"metadata":{}},{"name":"stdout","text":"⚠️ RecipeBERT not found. Using BERT instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"635e433b0e694b8a84fc5da56b6c373a"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n🔁 Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2491/2491 [29:47<00:00,  1.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Train Loss: 2.8454\n✅ Train Accuracy: 38.42%\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 623/623 [02:22<00:00,  4.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"🧪 Val Loss: 2.1018\n🧪 Val Accuracy: 48.99%\n\n🔁 Epoch 2/5\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2491/2491 [28:51<00:00,  1.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Train Loss: 1.9134\n✅ Train Accuracy: 52.21%\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 623/623 [02:20<00:00,  4.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"🧪 Val Loss: 1.7955\n🧪 Val Accuracy: 54.06%\n\n🔁 Epoch 3/5\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2491/2491 [30:03<00:00,  1.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Train Loss: 1.6228\n✅ Train Accuracy: 57.26%\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 623/623 [02:20<00:00,  4.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"🧪 Val Loss: 1.6685\n🧪 Val Accuracy: 55.92%\n\n🔁 Epoch 4/5\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2491/2491 [28:51<00:00,  1.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Train Loss: 1.4390\n✅ Train Accuracy: 61.46%\n","output_type":"stream"},{"name":"stderr","text":"Training:  14%|█▎        | 337/2491 [03:54<24:56,  1.44it/s]]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"c","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T19:53:59.377819Z","iopub.execute_input":"2025-04-09T19:53:59.378112Z"}},"outputs":[{"name":"stdout","text":"🔥 Using device: cuda\n⚠️ RecipeBERT not found. Using BERT instead.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n🔁 Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2491/2491 [29:24<00:00,  1.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Train Loss: 2.8796\n✅ Train Accuracy: 37.02%\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 623/623 [02:20<00:00,  4.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"🧪 Val Loss: 2.1365\n🧪 Val Accuracy: 48.74%\n\n🔁 Epoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2491/2491 [29:20<00:00,  1.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Train Loss: 1.9357\n✅ Train Accuracy: 51.62%\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 623/623 [02:30<00:00,  4.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"🧪 Val Loss: 1.7862\n🧪 Val Accuracy: 53.88%\n\n🔁 Epoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2491/2491 [29:15<00:00,  1.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Train Loss: 1.6372\n✅ Train Accuracy: 57.16%\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 623/623 [02:20<00:00,  4.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"🧪 Val Loss: 1.6760\n🧪 Val Accuracy: 55.63%\n\n🔁 Epoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2491/2491 [29:36<00:00,  1.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Train Loss: 1.4508\n✅ Train Accuracy: 61.26%\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 623/623 [02:30<00:00,  4.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"🧪 Val Loss: 1.6299\n🧪 Val Accuracy: 56.32%\n\n🔁 Epoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2491/2491 [29:01<00:00,  1.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Train Loss: 1.3315\n✅ Train Accuracy: 64.07%\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 623/623 [02:20<00:00,  4.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"🧪 Val Loss: 1.6156\n🧪 Val Accuracy: 57.15%\n\n🔁 Epoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 2491/2491 [29:53<00:00,  1.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"📉 Train Loss: 1.2883\n✅ Train Accuracy: 65.56%\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 623/623 [02:26<00:00,  4.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"🧪 Val Loss: 1.6156\n🧪 Val Accuracy: 57.15%\n\n🔁 Epoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training:  96%|█████████▌| 2391/2491 [27:41<01:09,  1.45it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom datasets import Dataset\nfrom transformers import DistilBertTokenizerFast, RobertaTokenizerFast\n\n# Label encoding\nle = LabelEncoder()\ndf['label'] = le.fit_transform(df['Category'])\n\n# Convert to Hugging Face Dataset\ndataset = Dataset.from_pandas(df[['text', 'label']])\n\n# Load tokenizers\ndistilbert_tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\nroberta_tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n\n# Define tokenization functions\ndef tokenize_distilbert(example):\n    return distilbert_tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n\ndef tokenize_roberta(example):\n    return roberta_tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n\n# Tokenize\ntokenized_distilbert = dataset.map(tokenize_distilbert, batched=True)\ntokenized_roberta = dataset.map(tokenize_roberta, batched=True)\n\n# Split into train/test\ntokenized_distilbert = tokenized_distilbert.train_test_split(test_size=0.2, seed=42)\ntokenized_roberta = tokenized_roberta.train_test_split(test_size=0.2, seed=42)\n\n# Remove unnecessary columns (for Trainer compatibility)\ntokenized_distilbert = tokenized_distilbert.remove_columns(['text'])\ntokenized_roberta = tokenized_roberta.remove_columns(['text'])\n\n# Set format for PyTorch\ntokenized_distilbert.set_format(\"torch\")\ntokenized_roberta.set_format(\"torch\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T08:04:05.943051Z","iopub.execute_input":"2025-04-14T08:04:05.943407Z","iopub.status.idle":"2025-04-14T08:04:45.613855Z","shell.execute_reply.started":"2025-04-14T08:04:05.943376Z","shell.execute_reply":"2025-04-14T08:04:45.613178Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8561418adc0444f794ea6c0e7cacaa36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb4187f722834fd7bd2389d745e2a7e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33f8c1cab6f4497fb5cc22ef2fb73a4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d8317012ae34d17939addd7e06f1ae1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"809919e3e9824798b47e11a459d3bfdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b37cd89510c466aad803e6e74494f58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4abecfcfea10466194b9126b3fea86f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c6303f416bd4bdb9675b817972b18b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c94aa869f37a4a2ea93f5b08e5233561"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50012 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da5dff89eda743e2a33fb57447e6382b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50012 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db500aeca19f40f28d228b1e4ea2c2dd"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"\n# 2. LOAD AND PREP DATA\n# Assuming you've already loaded your data into `df` and cleaned it\n# And you've created a combined text field already\ndf['text'] = df['Recipe name'].fillna('') + ' ' + \\\n             df['List of ingredients'].fillna('') + ' ' + \\\n             df['List of instructions'].fillna('')\n\n# Label encoding\nle = LabelEncoder()\ndf['label'] = le.fit_transform(df['Category'])\n\n# Create Hugging Face Dataset\ndataset = Dataset.from_pandas(df[['text', 'label']])\n\n# 3. TOKENIZATION\ntokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n\ndef tokenize(example):\n    return tokenizer(example['text'], padding='max_length', truncation=True, max_length=256)\n\ntokenized_dataset = dataset.map(tokenize, batched=True)\n\n# Remove text column and set format for PyTorch\ntokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\ntokenized_dataset.set_format(\"torch\")\n\n# Train/test split\ntokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n\n# 4. LOAD ROBERTA MODEL\nmodel = RobertaForSequenceClassification.from_pretrained(\n    \"roberta-base\",\n    num_labels=len(le.classes_)\n)\n\n# 5. TRAINING ARGUMENTS\ntraining_args = TrainingArguments(\n    output_dir=\"./roberta-results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir=\"./logs_roberta\",\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\"\n)\n\n# 6. METRICS\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=1)\n    return {\n        'accuracy': accuracy_score(labels, predictions),\n        'f1': f1_score(labels, predictions, average='weighted')\n    }\n\n# 7. TRAINER\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['test'],\n    compute_metrics=compute_metrics\n)\n\n# 8. TRAIN\ntrainer.train()\n\n# 9. EVALUATION\ndef evaluate_model(trainer, tokenized_dataset, model_name=\"Model\"):\n    print(f\"\\n📊 Evaluation for {model_name}\")\n    preds_output = trainer.predict(tokenized_dataset['test'])\n    preds = np.argmax(preds_output.predictions, axis=1)\n    labels = preds_output.label_ids\n\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average='weighted')\n    print(\"✅ Accuracy:\", acc)\n    print(\"✅ F1 Score:\", f1)\n\n    print(\"\\n📝 Classification Report:\")\n    print(classification_report(labels, preds, target_names=le.classes_))\n\n    cm = confusion_matrix(labels, preds)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=le.classes_, yticklabels=le.classes_)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(f\"{model_name} - Confusion Matrix\")\n    plt.show()\n\n    return acc, f1\n\n# Run evaluation\nevaluate_model(trainer, tokenized_dataset, \"RoBERTa\")\n\n# 10. LOSS PLOT\ndef plot_loss(trainer, model_name=\"Model\"):\n    logs = trainer.state.log_history\n    train_loss, eval_loss, steps = [], [], []\n\n    for log in logs:\n        if 'loss' in log:\n            train_loss.append(log['loss'])\n            steps.append(log['step'])\n        if 'eval_loss' in log:\n            eval_loss.append(log['eval_loss'])\n\n    plt.figure(figsize=(10, 5))\n    plt.plot(steps[:len(train_loss)], train_loss, label=\"Training Loss\")\n    if eval_loss:\n        plt.plot(steps[:len(eval_loss)], eval_loss, label=\"Validation Loss\")\n    plt.xlabel(\"Steps\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"{model_name} - Loss Curve\")\n    plt.legend()\n    plt.grid()\n    plt.show()\n\n# Plot loss\nplot_loss(trainer, \"RoBERTa\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T08:06:48.911888Z","iopub.execute_input":"2025-04-14T08:06:48.912355Z","iopub.status.idle":"2025-04-14T08:16:52.865298Z","shell.execute_reply.started":"2025-04-14T08:06:48.912318Z","shell.execute_reply":"2025-04-14T08:16:52.863791Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50012 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf13f55da05f4318bc7af680e5473abf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d742ec7896c8408db83ef4113c076766"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-4189702c702f>\u001b[0m in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m# 8. TRAIN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# 9. EVALUATION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2427\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2428\u001b[0m         \u001b[0mgrad_norm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2429\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2431\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_on_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_training_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_train_begin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             result = getattr(callback, event)(\n\u001b[0m\u001b[1;32m    520\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m                 self._wandb.init(\n\u001b[0m\u001b[1;32m    838\u001b[0m                     \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WANDB_PROJECT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"huggingface\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0minit_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0mwi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_WandbInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m         wi.setup(\n\u001b[0m\u001b[1;32m   1291\u001b[0m             \u001b[0minit_settings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, init_settings, config, config_exclude_keys, config_include_keys, allow_val_change, monitor_gym)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_noop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             wandb_login._login(\n\u001b[0m\u001b[1;32m    290\u001b[0m                 \u001b[0manonymous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manonymous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, _backend, _silent, _disable_warning, _entity)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;34m\"\"\"Updates the global API key by prompting the user.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mApiKeyStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             directive = (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m                 key = apikey.prompt_api_key(\n\u001b[0m\u001b[1;32m    244\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                     \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/apikey.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mjupyter\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"google.colab\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mlog_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOG_STRING_NOCOLOR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjupyter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattempt_colab_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp_url\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mwrite_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mattempt_colab_login\u001b[0;34m(app_url)\u001b[0m\n\u001b[1;32m    334\u001b[0m     )\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_wandbApiKey\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"# 1. IMPORTS\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import (\n    DistilBertTokenizerFast,\n    DistilBertForSequenceClassification,\n    TrainingArguments,\n    Trainer\n)\n\n# 2. LOAD AND PREP DATA\n# Reuse your existing `df` with combined text\ndf['text'] = df['Recipe name'].fillna('') + ' ' + \\\n             df['List of ingredients'].fillna('') + ' ' + \\\n             df['List of instructions'].fillna('')\n\n# Label encoding\nle = LabelEncoder()\ndf['label'] = le.fit_transform(df['Category'])\n\n# Create Hugging Face Dataset\ndataset = Dataset.from_pandas(df[['text', 'label']])\n\n# 3. TOKENIZATION\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n\ndef tokenize(example):\n    return tokenizer(example['text'], padding='max_length', truncation=True, max_length=256)\n\ntokenized_dataset = dataset.map(tokenize, batched=True)\n\n# Remove text column and set format for PyTorch\ntokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\ntokenized_dataset.set_format(\"torch\")\n\n# Train/test split\ntokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n\n# 4. LOAD DISTILBERT MODEL\nmodel = DistilBertForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\",\n    num_labels=len(le.classes_)\n)\n\n# 5. TRAINING ARGUMENTS\ntraining_args = TrainingArguments(\n    output_dir=\"./distilbert-results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir=\"./logs_distilbert\",\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\"\n)\n\n# 6. METRICS\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=1)\n    return {\n        'accuracy': accuracy_score(labels, predictions),\n        'f1': f1_score(labels, predictions, average='weighted')\n    }\n\n# 7. TRAINER\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['test'],\n    compute_metrics=compute_metrics\n)\n\n# 8. TRAIN\ntrainer.train()\n\n# 9. EVALUATION FUNCTION\ndef evaluate_model(trainer, tokenized_dataset, model_name=\"Model\"):\n    print(f\"\\n📊 Evaluation for {model_name}\")\n    preds_output = trainer.predict(tokenized_dataset['test'])\n    preds = np.argmax(preds_output.predictions, axis=1)\n    labels = preds_output.label_ids\n\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average='weighted')\n    print(\"✅ Accuracy:\", acc)\n    print(\"✅ F1 Score:\", f1)\n\n    print(\"\\n📝 Classification Report:\")\n    print(classification_report(labels, preds, target_names=le.classes_))\n\n    cm = confusion_matrix(labels, preds)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n                xticklabels=le.classes_, yticklabels=le.classes_)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(f\"{model_name} - Confusion Matrix\")\n    plt.show()\n\n    return acc, f1\n\n# Run evaluation\nevaluate_model(trainer, tokenized_dataset, \"DistilBERT\")\n\n# 10. LOSS PLOT FUNCTION\ndef plot_loss(trainer, model_name=\"Model\"):\n    logs = trainer.state.log_history\n    train_loss, eval_loss, steps = [], [], []\n\n    for log in logs:\n        if 'loss' in log:\n            train_loss.append(log['loss'])\n            steps.append(log['step'])\n        if 'eval_loss' in log:\n            eval_loss.append(log['eval_loss'])\n\n    plt.figure(figsize=(10, 5))\n    plt.plot(steps[:len(train_loss)], train_loss, label=\"Training Loss\")\n    if eval_loss:\n        plt.plot(steps[:len(eval_loss)], eval_loss, label=\"Validation Loss\")\n    plt.xlabel(\"Steps\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"{model_name} - Loss Curve\")\n    plt.legend()\n    plt.grid()\n    plt.show()\n\n# Plot loss\nplot_loss(trainer, \"DistilBERT\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T08:18:00.564965Z","iopub.execute_input":"2025-04-14T08:18:00.565277Z","iopub.status.idle":"2025-04-14T08:19:33.226593Z","shell.execute_reply.started":"2025-04-14T08:18:00.565248Z","shell.execute_reply":"2025-04-14T08:19:33.225446Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13ade842f90a4bcfbe95a95db8764a72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70cd74209a8d465db75cb0f0b7a624b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b564b0cc16fa43fe8fba48001468bea2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae3347f136cf42a19d2575dc1abf5a3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50012 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a93cae27df6f4199927b0ac807c56d19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea659a44d6cc4e569bcc843a76d38a61"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-e5bca4db1134>\u001b[0m in \u001b[0;36m<cell line: 84>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# 8. TRAIN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# 9. EVALUATION FUNCTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2427\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2428\u001b[0m         \u001b[0mgrad_norm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2429\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2431\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_on_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_training_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_train_begin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             result = getattr(callback, event)(\n\u001b[0m\u001b[1;32m    520\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m                 self._wandb.init(\n\u001b[0m\u001b[1;32m    838\u001b[0m                     \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WANDB_PROJECT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"huggingface\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0minit_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0mwi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_WandbInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m         wi.setup(\n\u001b[0m\u001b[1;32m   1291\u001b[0m             \u001b[0minit_settings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, init_settings, config, config_exclude_keys, config_include_keys, allow_val_change, monitor_gym)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_noop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             wandb_login._login(\n\u001b[0m\u001b[1;32m    290\u001b[0m                 \u001b[0manonymous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manonymous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, _backend, _silent, _disable_warning, _entity)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;34m\"\"\"Updates the global API key by prompting the user.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mApiKeyStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             directive = (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m                 key = apikey.prompt_api_key(\n\u001b[0m\u001b[1;32m    244\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                     \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/apikey.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mjupyter\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"google.colab\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mlog_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOG_STRING_NOCOLOR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjupyter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattempt_colab_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp_url\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mwrite_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mattempt_colab_login\u001b[0;34m(app_url)\u001b[0m\n\u001b[1;32m    334\u001b[0m     )\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_wandbApiKey\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"# 1. IMPORTS\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import (\n    RobertaTokenizerFast,\n    RobertaForSequenceClassification,\n    TrainingArguments,\n    Trainer\n)\n\n\n\n# Combine the text fields into a single column\ndf['text'] = df['Recipe name'].fillna('') + ' ' + \\\n             df['List of ingredients'].fillna('') + ' ' + \\\n             df['List of instructions'].fillna('')\n\n# Encode the 'Category' column into numeric labels\nle = LabelEncoder()\ndf['label'] = le.fit_transform(df['Category'])\n\n# Confirm\ndf[['text', 'label']].head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T09:17:10.230295Z","iopub.execute_input":"2025-04-14T09:17:10.230618Z","iopub.status.idle":"2025-04-14T09:17:13.128697Z","shell.execute_reply.started":"2025-04-14T09:17:10.230596Z","shell.execute_reply":"2025-04-14T09:17:13.127746Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"                                                text  label\n0  cheesy grits casserole 4cupswater 1teaspoonsal...    407\n1  crock pot beer brats 8bratwursts 2tablespoonso...    441\n2  banana bread with coconut milk 4ripe mashedban...    450\n3  pasta salad supreme 16ouncesrotini pastaor 16 ...    577\n4  bread machine monkey pull aparts 112teaspoonsa...     51","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cheesy grits casserole 4cupswater 1teaspoonsal...</td>\n      <td>407</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>crock pot beer brats 8bratwursts 2tablespoonso...</td>\n      <td>441</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>banana bread with coconut milk 4ripe mashedban...</td>\n      <td>450</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>pasta salad supreme 16ouncesrotini pastaor 16 ...</td>\n      <td>577</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bread machine monkey pull aparts 112teaspoonsa...</td>\n      <td>51</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"from transformers import RobertaTokenizerFast\n\n# Load RoBERTa tokenizer\nroberta_tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n\n# Create Hugging Face Dataset\ndataset = Dataset.from_pandas(df[['text', 'label']])\n\n# Tokenize the dataset\ndef tokenize(example):\n    return roberta_tokenizer(example['text'], padding='max_length', truncation=True, max_length=256)\n\ntokenized_dataset = dataset.map(tokenize, batched=True)\n\n# Remove 'text' column (not needed for training) and set format for PyTorch\ntokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\ntokenized_dataset.set_format(\"torch\")\n\n# Split into train/test sets\ntokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n\n# Preview\ntokenized_dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T09:17:17.719180Z","iopub.execute_input":"2025-04-14T09:17:17.719825Z","iopub.status.idle":"2025-04-14T09:17:35.107251Z","shell.execute_reply.started":"2025-04-14T09:17:17.719797Z","shell.execute_reply":"2025-04-14T09:17:35.106419Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7183be78745410595788ceb2c00e9c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e86db869ac14302a8a85dd9c804d8bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"949ac20e09944e60a32f5704f36c22a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25e295d5c06242eeb54b72fc9dcf59b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d515ff2242b4f36a393a43487a148bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50012 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03395bdc8f104b50abe800267d7cadf6"}},"metadata":{}},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['label', 'input_ids', 'attention_mask'],\n        num_rows: 40009\n    })\n    test: Dataset({\n        features: ['label', 'input_ids', 'attention_mask'],\n        num_rows: 10003\n    })\n})"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"from transformers import RobertaForSequenceClassification, TrainingArguments, Trainer\n\n# Load the RoBERTa model for classification\nmodel = RobertaForSequenceClassification.from_pretrained(\n    \"roberta-base\",\n    num_labels=len(le.classes_)  # Number of unique recipe categories\n)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./roberta_output\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=9,\n    weight_decay=0.01,\n    logging_dir=\"./roberta_logs\",\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T09:17:38.935816Z","iopub.execute_input":"2025-04-14T09:17:38.936131Z","iopub.status.idle":"2025-04-14T09:17:42.049175Z","shell.execute_reply.started":"2025-04-14T09:17:38.936106Z","shell.execute_reply":"2025-04-14T09:17:42.048510Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07e06492881c41a28d82bae4369c72b8"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\n\n# Define metric function for evaluation\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=1)\n    return {\n        'accuracy': accuracy_score(labels, predictions),\n        'f1': f1_score(labels, predictions, average='weighted')\n    }\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['test'],\n    compute_metrics=compute_metrics\n)\n\n# Start training 🚀\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T09:17:47.067510Z","iopub.execute_input":"2025-04-14T09:17:47.067810Z","iopub.status.idle":"2025-04-14T10:14:52.660464Z","shell.execute_reply.started":"2025-04-14T09:17:47.067787Z","shell.execute_reply":"2025-04-14T10:14:52.659695Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3753' max='3753' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3753/3753 57:01, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.018300</td>\n      <td>1.947674</td>\n      <td>0.508347</td>\n      <td>0.449914</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.630500</td>\n      <td>1.707289</td>\n      <td>0.551635</td>\n      <td>0.492324</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.638700</td>\n      <td>1.625148</td>\n      <td>0.570629</td>\n      <td>0.529646</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3753, training_loss=1.99568143311482, metrics={'train_runtime': 3424.8741, 'train_samples_per_second': 35.046, 'train_steps_per_second': 1.096, 'total_flos': 1.5873720190336512e+16, 'train_loss': 1.99568143311482, 'epoch': 3.0})"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./roberta_output\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=9,  # ⬅️ Increase to desired total epochs\n    weight_decay=0.01,\n    logging_dir=\"./roberta_logs\",\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\"\n)\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['test'],\n    compute_metrics=compute_metrics\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T10:19:27.334146Z","iopub.execute_input":"2025-04-14T10:19:27.334496Z","iopub.status.idle":"2025-04-14T10:19:27.381283Z","shell.execute_reply.started":"2025-04-14T10:19:27.334469Z","shell.execute_reply":"2025-04-14T10:19:27.380309Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"trainer.train(resume_from_checkpoint=True)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T10:20:13.217155Z","iopub.execute_input":"2025-04-14T10:20:13.217529Z","iopub.status.idle":"2025-04-14T12:13:59.390467Z","shell.execute_reply.started":"2025-04-14T10:20:13.217500Z","shell.execute_reply":"2025-04-14T12:13:59.389530Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3081: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint_rng_state = torch.load(rng_file)\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='11259' max='11259' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [11259/11259 1:53:43, Epoch 9/9]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>4</td>\n      <td>1.463700</td>\n      <td>1.635222</td>\n      <td>0.563931</td>\n      <td>0.523022</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.330200</td>\n      <td>1.586402</td>\n      <td>0.570329</td>\n      <td>0.522752</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.243900</td>\n      <td>1.566359</td>\n      <td>0.571928</td>\n      <td>0.544195</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.072100</td>\n      <td>1.577766</td>\n      <td>0.577127</td>\n      <td>0.540842</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.097500</td>\n      <td>1.591536</td>\n      <td>0.582025</td>\n      <td>0.559292</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.950200</td>\n      <td>1.602031</td>\n      <td>0.583225</td>\n      <td>0.566046</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=11259, training_loss=0.8088135231408379, metrics={'train_runtime': 6824.25, 'train_samples_per_second': 52.765, 'train_steps_per_second': 1.65, 'total_flos': 4.762116057100954e+16, 'train_loss': 0.8088135231408379, 'epoch': 9.0})"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"# Install required libraries\n!pip install -q transformers datasets scikit-learn\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\nimport numpy as np\nimport torch\nfrom sklearn.preprocessing import LabelEncoder\n\n# Label Encoding\nlabel_encoder = LabelEncoder()\ndf['label'] = label_encoder.fit_transform(df['Category'])\nnum_labels = len(label_encoder.classes_)\nprint(\"✅ Number of categories:\", num_labels)\n\n# Split dataset\ntrain_df, test_df = train_test_split(df[['text', 'label']], test_size=0.2, random_state=42)\n\n# Convert to Hugging Face dataset format\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Load RecipeBERT\n# model_name = \"recipegpt/recipe-bert\"\n# model_name = \"tae898/recipe-bert\"\nmodel_name = \"alexdseo/RecipeBERT\"  # Verified Hugging Face model\n\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels).to(device)\n\n# Tokenization function\ndef tokenize_function(example):\n    return tokenizer(\n        example[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512,\n    )\n\n# Apply tokenization\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# Set format for PyTorch\ntrain_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\ntest_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    save_total_limit=2\n)\n\n# Compute accuracy\nfrom sklearn.metrics import accuracy_score\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=1)\n    return {\"accuracy\": accuracy_score(labels, predictions)}\n\n# Trainer setup\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\n# Train the model\ntrainer.train()\n\n# Save model\ntrainer.save_model(\"/kaggle/working/recipebert-finetuned\")\ntokenizer.save_pretrained(\"/kaggle/working/recipebert-finetuned\")\n\nprint(\"✅ Training complete. Model saved to /kaggle/working/recipebert-finetuned\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T06:15:31.564575Z","iopub.execute_input":"2025-04-15T06:15:31.564982Z","iopub.status.idle":"2025-04-15T08:20:51.379448Z","shell.execute_reply.started":"2025-04-15T06:15:31.564949Z","shell.execute_reply":"2025-04-15T08:20:51.378685Z"}},"outputs":[{"name":"stdout","text":"✅ Number of categories: 591\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52e0eb36a7b34c83a6ffd8dc46953290"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ecb21f46ffd47fdaa8e96dba8855257"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"632e086a6f6845019b5a9e7a7b9b91d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b26e30ed38949d89b6b2c742a6b48e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/664 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f56690b01e54150b395f868b0192f3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d7bcae653434666a8aa18d3d107875a"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alexdseo/RecipeBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40009 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ed06c793a4844b388cc84da18930c6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10003 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17577094a3ae4f5fad57656def8f6c43"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n<ipython-input-9-85a99c8524b8>:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7503' max='7503' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7503/7503 2:04:41, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.587700</td>\n      <td>1.711424</td>\n      <td>0.553534</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.584800</td>\n      <td>1.534349</td>\n      <td>0.579426</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.091700</td>\n      <td>1.494592</td>\n      <td>0.591722</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Training complete. Model saved to /kaggle/working/recipebert-finetuned\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"**4**","metadata":{}},{"cell_type":"code","source":"# ✅ Install required libraries\n!pip install -q transformers datasets scikit-learn\n\n# ✅ Imports\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\nimport numpy as np\nimport torch\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\n\n# ✅ Label Encoding\nlabel_encoder = LabelEncoder()\ndf['label'] = label_encoder.fit_transform(df['Category'])\nnum_labels = len(label_encoder.classes_)\nprint(\"✅ Number of categories:\", num_labels)\n\n# ✅ Split dataset\ntrain_df, test_df = train_test_split(df[['text', 'label']], test_size=0.2, random_state=42)\n\n# ✅ Convert to Hugging Face dataset format\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# ✅ Load RecipeBERT model\nmodel_name = \"alexdseo/RecipeBERT\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n\n# ✅ Tokenization function\ndef tokenize_function(example):\n    return tokenizer(\n        example[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=512,\n    )\n\n# ✅ Apply tokenization\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# ✅ Set format for PyTorch\ntrain_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\ntest_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\n# ✅ Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=9,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    save_total_limit=2\n)\n\n# ✅ Accuracy metric\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=1)\n    return {\"accuracy\": accuracy_score(labels, predictions)}\n\n# ✅ Trainer setup\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n)\n\n# ✅ Train the model\ntrainer.train()\n\n# ✅ Save the model\ntrainer.save_model(\"/kaggle/working/recipebert-finetuned\")\ntokenizer.save_pretrained(\"/kaggle/working/recipebert-finetuned\")\nprint(\"✅ Training complete. Model saved to /kaggle/working/recipebert-finetuned\")\n\n# ✅ Evaluate on test set\ntest_results = trainer.evaluate(test_dataset)\nprint(f\"\\n✅ Final Test Accuracy: {test_results['eval_accuracy']:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T08:38:45.288525Z","iopub.execute_input":"2025-04-15T08:38:45.289004Z","iopub.status.idle":"2025-04-15T12:52:03.927728Z","shell.execute_reply.started":"2025-04-15T08:38:45.288965Z","shell.execute_reply":"2025-04-15T12:52:03.926964Z"}},"outputs":[{"name":"stdout","text":"✅ Number of categories: 591\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alexdseo/RecipeBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40009 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3af32e02bd274cf8b6b3f56ee65f2586"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10003 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4e6e32ebd0445cb9c2543323fa83db4"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n<ipython-input-10-f2803cc449de>:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15006' max='22509' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15006/22509 4:09:38 < 2:04:50, 1.00 it/s, Epoch 6/9]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.560900</td>\n      <td>1.695374</td>\n      <td>0.555733</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.604000</td>\n      <td>1.555099</td>\n      <td>0.569229</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.101700</td>\n      <td>1.530210</td>\n      <td>0.585424</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.958500</td>\n      <td>1.526386</td>\n      <td>0.599820</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.868800</td>\n      <td>1.594048</td>\n      <td>0.593922</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.787700</td>\n      <td>1.710592</td>\n      <td>0.598620</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Training complete. Model saved to /kaggle/working/recipebert-finetuned\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='626' max='626' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [626/626 03:07]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\n✅ Final Test Accuracy: 0.5998\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Final Kaggle-compatible, GPU-ready script with rare label removal, accuracy printing, and recipe classification setup\n\nfinal_kaggle_ready_code = \"\"\"\n# ==================== RecipeBERT Fine-Tuning with Accuracy Output ===================='\"\"\"\n\n!pip install -q transformers datasets scikit-learn\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback, TrainerCallback\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\nimport numpy as np\nimport torch\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\n\n# ✅ Combine structured text\ndf['text'] = (\n    \"Recipe: \" + df['Recipe name'].fillna('') +\n    \". Ingredients: \" + df['List of ingredients'].fillna('') +\n    \". Instructions: \" + df['List of instructions'].fillna('')\n)\n\n# ✅ Encode labels\nlabel_encoder = LabelEncoder()\ndf['label'] = label_encoder.fit_transform(df['Category'])\n\n# ✅ Drop classes with <2 samples to enable stratification\nlabel_counts = df['label'].value_counts()\nvalid_labels = label_counts[label_counts > 1].index\ndf = df[df['label'].isin(valid_labels)].reset_index(drop=True)\n\n# ✅ Train/test split\ntrain_df, test_df = train_test_split(df[['text', 'label']], test_size=0.2, random_state=42, stratify=df['label'])\n\n# ✅ Convert to Hugging Face Dataset format\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# ✅ Load RecipeBERT\nmodel_name = \"roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n\n# ✅ Tokenization\ndef tokenize_function(example):\n    return tokenizer(\n        example[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=256,\n    )\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\ntrain_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\ntest_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\n# ✅ Metric\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=1)\n    return {\"accuracy\": accuracy_score(labels, predictions)}\n\n# ✅ Callback to print accuracy\nclass PrintMetricsCallback(TrainerCallback):\n    def on_epoch_end(self, args, state, control, **kwargs):\n        logs = state.log_history[-1]\n        if \"eval_accuracy\" in logs:\n            print(f\"📈 Epoch {int(state.epoch)} - Validation Accuracy: {logs['eval_accuracy']:.4f}\")\n\n# ✅ Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=5,\n    learning_rate=3e-5,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    save_total_limit=2,\n    warmup_steps=100,\n    fp16=True  # Enable mixed precision if on GPU\n)\n\n# ✅ Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=1), PrintMetricsCallback()]\n)\n\n# ✅ Train\ntrainer.train()\n\n# ✅ Save model\ntrainer.save_model(\"/kaggle/working/recipebert-finetuned-v2\")\ntokenizer.save_pretrained(\"/kaggle/working/recipebert-finetuned-v2\")\n\n# ✅ Final Evaluation\ntest_results = trainer.evaluate(test_dataset)\nprint(f\"✅ Final Test Accuracy: {test_results['eval_accuracy']:.4f}\")\n\n\n# Save the corrected and complete training script\nfile_path = \"/mnt/data/kaggle_recipebert_training_final.py\"\nwith open(file_path, \"w\") as f:\n    f.write(final_kaggle_ready_code)\n\nfile_path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T23:21:56.193657Z","iopub.execute_input":"2025-04-16T23:21:56.193962Z","iopub.status.idle":"2025-04-17T01:16:28.323623Z","shell.execute_reply.started":"2025-04-16T23:21:56.193939Z","shell.execute_reply":"2025-04-17T01:16:28.322058Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5edff6aca784f30a7c3d18bf8203eb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df07c73c62b94bbb93337534dc0e0e4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6e73d1a97a04ab8ae4e43be9c57ab45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4402979498dc491682c89611557ff147"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdc811993d3948598538e148aa5ffe92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3212b72bd0404e75bf8e9b6907b59693"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/39844 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"add207276bf8481e87b3dec62ba59470"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9961 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9045fb5d677a40d39d97f8022518d3e8"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n<ipython-input-9-c4559abf44b2>:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12455' max='12455' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12455/12455 1:52:25, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.098300</td>\n      <td>1.892296</td>\n      <td>0.516414</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.690900</td>\n      <td>1.640451</td>\n      <td>0.553559</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.468300</td>\n      <td>1.531178</td>\n      <td>0.576247</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.344700</td>\n      <td>1.503520</td>\n      <td>0.581167</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.173100</td>\n      <td>1.501380</td>\n      <td>0.585383</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='623' max='623' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [623/623 01:41]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"✅ Final Test Accuracy: 0.5854\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-c4559abf44b2>\u001b[0m in \u001b[0;36m<cell line: 115>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;31m# Save the corrected and complete training script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/mnt/data/kaggle_recipebert_training_final.py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_kaggle_ready_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/kaggle_recipebert_training_final.py'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/mnt/data/kaggle_recipebert_training_final.py'","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3****** \"roberta-base\"","metadata":{}},{"cell_type":"code","source":"# Final Kaggle-compatible, GPU-ready script with rare label removal, accuracy printing, and recipe classification setup\n\nfinal_kaggle_ready_code = \"\"\"\n# ==================== RecipeBERT Fine-Tuning with Accuracy Output ===================='\"\"\"\n\n!pip install -q transformers datasets scikit-learn\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback, TrainerCallback\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\nimport numpy as np\nimport torch\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\n\n# ✅ Combine structured text\ndf['text'] = (\n    \"Recipe: \" + df['Recipe name'].fillna('') +\n    \". Ingredients: \" + df['List of ingredients'].fillna('') +\n    \". Instructions: \" + df['List of instructions'].fillna('')\n)\n\n# ✅ Encode labels\nlabel_encoder = LabelEncoder()\ndf['label'] = label_encoder.fit_transform(df['Category'])\n\n# ✅ Drop classes with <2 samples to enable stratification\nlabel_counts = df['label'].value_counts()\nvalid_labels = label_counts[label_counts > 1].index\ndf = df[df['label'].isin(valid_labels)].reset_index(drop=True)\n\n# ✅ Train/test split\ntrain_df, test_df = train_test_split(df[['text', 'label']], test_size=0.2, random_state=42, stratify=df['label'])\n\n# ✅ Convert to Hugging Face Dataset format\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# ✅ Load RecipeBERT\nmodel_name = \"roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n\n# ✅ Tokenization\ndef tokenize_function(example):\n    return tokenizer(\n        example[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=256,\n    )\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\ntrain_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\ntest_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\n# ✅ Metric\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=1)\n    return {\"accuracy\": accuracy_score(labels, predictions)}\n\n# ✅ Callback to print accuracy\nclass PrintMetricsCallback(TrainerCallback):\n    def on_epoch_end(self, args, state, control, **kwargs):\n        logs = state.log_history[-1]\n        if \"eval_accuracy\" in logs:\n            print(f\"📈 Epoch {int(state.epoch)} - Validation Accuracy: {logs['eval_accuracy']:.4f}\")\n\n# ✅ Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=10,\n    learning_rate=3e-5,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    save_total_limit=2,\n    warmup_steps=100,\n    fp16=True  # Enable mixed precision if on GPU\n)\n\n# ✅ Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=1), PrintMetricsCallback()]\n)\n\n# ✅ Train\ntrainer.train()\n\n# ✅ Save model\ntrainer.save_model(\"/kaggle/working/recipebert-finetuned-v2\")\ntokenizer.save_pretrained(\"/kaggle/working/recipebert-finetuned-v2\")\n\n# ✅ Final Evaluation\ntest_results = trainer.evaluate(test_dataset)\nprint(f\"✅ Final Test Accuracy: {test_results['eval_accuracy']:.4f}\")\n\n\n# Save the corrected and complete training script\nfile_path = \"/mnt/data/kaggle_recipebert_training_final.py\"\nwith open(file_path, \"w\") as f:\n    f.write(final_kaggle_ready_code)\n\nfile_path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T01:21:46.504260Z","iopub.execute_input":"2025-04-17T01:21:46.504603Z","iopub.status.idle":"2025-04-17T04:23:33.297602Z","shell.execute_reply.started":"2025-04-17T01:21:46.504578Z","shell.execute_reply":"2025-04-17T04:23:33.296109Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/39844 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0009a897bdcc4756ad2a1d7e9d5d96f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9961 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbb6bc4830244059b86f7df7b44fcca8"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n<ipython-input-10-12b64bc53cb8>:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='19928' max='24910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [19928/24910 2:59:42 < 44:55, 1.85 it/s, Epoch 8/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.122000</td>\n      <td>1.887083</td>\n      <td>0.512800</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.704200</td>\n      <td>1.652469</td>\n      <td>0.547234</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.493900</td>\n      <td>1.531836</td>\n      <td>0.568116</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.369700</td>\n      <td>1.502980</td>\n      <td>0.575946</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.203300</td>\n      <td>1.497248</td>\n      <td>0.576147</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.097400</td>\n      <td>1.504450</td>\n      <td>0.585885</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.986700</td>\n      <td>1.522902</td>\n      <td>0.590804</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.877500</td>\n      <td>1.553591</td>\n      <td>0.590704</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='623' max='623' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [623/623 01:41]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"✅ Final Test Accuracy: 0.5908\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-12b64bc53cb8>\u001b[0m in \u001b[0;36m<cell line: 115>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;31m# Save the corrected and complete training script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/mnt/data/kaggle_recipebert_training_final.py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_kaggle_ready_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/kaggle_recipebert_training_final.py'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/mnt/data/kaggle_recipebert_training_final.py'","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T22:55:06.767291Z","iopub.execute_input":"2025-04-16T22:55:06.767589Z","iopub.status.idle":"2025-04-16T22:55:06.771350Z","shell.execute_reply.started":"2025-04-16T22:55:06.767568Z","shell.execute_reply":"2025-04-16T22:55:06.770400Z"}},"outputs":[],"execution_count":7}]}